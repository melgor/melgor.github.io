<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>BLCV</title><description>Thoughts, stories and ideas.</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>BLCV</title><link>http://localhost:2368/</link></image><generator>Ghost 1.8</generator><lastBuildDate>Sat, 07 Oct 2017 05:45:41 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Demystifying Face Recognition I: Baseline</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="testrnycharchitektursieci"&gt;Test różnych architektur sieci&lt;/h1&gt;
&lt;p&gt;Zgodnie z założeniami, mamy gotową bazę danych (CASIA-WebFace), zdefiniowany obraz wejściowy  (112x96) oraz DA jako &lt;code&gt;mirror&lt;/code&gt;, a całość będzie traktowana jako zadanie klasyfikacji. Czyli ostatnim elementem, który został nam to wybór architektektury. Ostatnie lata obfitowały w wiele różnorodnych technik, takich jak &lt;code&gt;ResNet&lt;/code&gt;, &lt;code&gt;Inception&lt;/code&gt;, &lt;code&gt;DenseNet&lt;/code&gt; czy &lt;code&gt;Inception-ResNet&lt;/code&gt;&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/demystifying-face-recognition-i-baseline/</link><guid isPermaLink="false">59d8661d71b26e116119e69d</guid><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Sat, 07 Oct 2017 05:29:36 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/10/faceresnet.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="testrnycharchitektursieci"&gt;Test różnych architektur sieci&lt;/h1&gt;
&lt;img src="http://localhost:2368/content/images/2017/10/faceresnet.png" alt="Demystifying Face Recognition I: Baseline"&gt;&lt;p&gt;Zgodnie z założeniami, mamy gotową bazę danych (CASIA-WebFace), zdefiniowany obraz wejściowy  (112x96) oraz DA jako &lt;code&gt;mirror&lt;/code&gt;, a całość będzie traktowana jako zadanie klasyfikacji. Czyli ostatnim elementem, który został nam to wybór architektektury. Ostatnie lata obfitowały w wiele różnorodnych technik, takich jak &lt;code&gt;ResNet&lt;/code&gt;, &lt;code&gt;Inception&lt;/code&gt;, &lt;code&gt;DenseNet&lt;/code&gt; czy &lt;code&gt;Inception-ResNet&lt;/code&gt;. Także w artykułach dotyczących Rozpoznawania Twarzy, autorzy proponowali własne wersje architektur. Obecnie skupimy się właśnie na tych ostatnich, głównie dlatego, że wiemy jaką jakość możemy z nich uzyskać oraz z powodu mniejszego liczby obliczeń. Przeanalizujemy także starsze architektury, aby pokazać czy struktury sieci mają aż takie znaczenie. Do dokładniejszych badań dotyczących architektury wrócimy, gdy przygotojemy dane wejściowe, operacje Data-Augumentation oraz funkcje celu (które także mogą służyć jako regularyzator).&lt;/p&gt;
&lt;h2 id="opisarchitektur"&gt;Opis Architektur&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"&gt;LeNet&lt;/a&gt;&lt;/strong&gt; - Pierwsza konwolucyjna architektura wykorzystywana do przetwarzania obrazów. Wejściowy obraz: 28x24.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf"&gt;DeepID&lt;/a&gt;&lt;/strong&gt; - Jedna z pierwszych wyspecjalizowanych sieć wykorzystywanych do ‘Rozpoznawania twarzy’. Moc została zwiększona zwiększenie liczby filtrów oraz połączenie cech z dwóch warstw konwolucyjnych. Wejściowy rozmiar: 42x36.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/1412.1265"&gt;DeepID2+&lt;/a&gt;&lt;/strong&gt; - Rozszerzona wersja DeepID, posiada jeszcze większą liczbę filtrów oraz rozmiar końcowych cech został zwiększony do 512. Wejściowy rozmiar: 56x48.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/1411.7923"&gt;Casia-Net&lt;/a&gt;&lt;/strong&gt; - Sieć zaproponowana po pierwszych sukcesach VGG i GoogLeNet w konkursie ImageNet. Wykorzystuje koncept rozmiarów kerneli 3x3 oraz ‘Averege Pooling’.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/1511.02683"&gt;Light-CNN&lt;/a&gt;&lt;/strong&gt;- Propozycja sieci wykorzystująca ‘MFM’ jako funkcję aktywacji. Autor wykazał, że sprawuje się o wiele lepiej niż ReLU, ELU czy PReLU.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/1611.08976"&gt;FaceResNet&lt;/a&gt;&lt;/strong&gt; - Sieć zaproponowana przez autorów artykułów CenterLoss i RangeLoss, która wykorzystuje połączenia ‘resiudualne’, tak jak w ‘ResNet’. Nie wykorzystuje warstw ‘BatchNorm’ oraz zastępuje funkcje aktywacji ‘Relu’ funkcjami ‘PRelu’.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/1704.08063"&gt;SphereFace&lt;/a&gt;&lt;/strong&gt; - Nowa wersja FaceResNet, którą główną zmianą jest usunięcie modułów MaxPool przez Konwolucje ze krokiem równym 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="http://www.yugangjiang.info/publication/icmr17-face.pdf"&gt;Fudan-Arch&lt;/a&gt;&lt;/strong&gt; - idea FaceResNet z BatchNorm. Autorzy zaproponowali dwie wersje sieci, Fast i Full model, różniącą się liczbą modułów w połączeniu residualnym.&lt;/p&gt;
&lt;p&gt;W większości przedstawonym powyżej architektur, jednym ze składników jest DropOut. Na obecnym etapie wolałbym uniknąć jego załączania, ale ze względu na walidację implementacji oraz możliwości sieci, każda z architektur zostanie przetestowana w dwóch wariantach: podstawowej oraz z Dropout. Do oceny każdej sieci wykorzystamy jej wyniki jakościowe (jak dokładność czy koszt) oraz parametry określajace czas do uzyskania wyniku. Model każdej z architekur został wybrany na podstawie modelu o najniższym walidacyjnym koszcie, czyli wynik LFW nie wpływał na wybór modelu, mimo że modele uzyskiwały lepsze rezultaty w innej epoce.&lt;/p&gt;
&lt;h2 id="wyniki"&gt;Wyniki&lt;/h2&gt;
&lt;iframe width="840" height="790" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/92.embed"&gt;&lt;/iframe&gt;
&lt;div&gt;
    &lt;a href="https://plot.ly/~melgor89/98/?share_key=VSxvwfjwCjfc4arfLtqkce" target="_blank" title="blog1-plot-pure" style="display: block; text-align: center;"&gt;&lt;img src="https://plot.ly/~melgor89/98.png?share_key=VSxvwfjwCjfc4arfLtqkce" alt="Demystifying Face Recognition I: Baseline" style="max-width: 100%;width: 800px;" width="800" onerror="this.onerror=null;this.src='https://plot.ly/404.png';"&gt;&lt;/a&gt;
    &lt;script data-plotly="melgor89:98" sharekey-plotly="VSxvwfjwCjfc4arfLtqkce" src="https://plot.ly/embed.js" async&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div&gt;
    &lt;a href="https://plot.ly/~melgor89/97/?share_key=Yn4LoxjBIG2zd42I8KMEfG" target="_blank" title="blog1-plot-dropout" style="display: block; text-align: center;"&gt;&lt;img src="https://plot.ly/~melgor89/97.png?share_key=Yn4LoxjBIG2zd42I8KMEfG" alt="Demystifying Face Recognition I: Baseline" style="max-width: 100%;width: 800px;" width="800" onerror="this.onerror=null;this.src='https://plot.ly/404.png';"&gt;&lt;/a&gt;
    &lt;script data-plotly="melgor89:97" sharekey-plotly="Yn4LoxjBIG2zd42I8KMEfG" src="https://plot.ly/embed.js" async&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;h2 id="ocenajakociarchitektur"&gt;Ocena jakości Architektur&lt;/h2&gt;
&lt;p&gt;Najpierw ocenimy architektury zawierające DropOut. Najlepszy wynik pod względem jakości na zbiorze walidacyjnym (~0.94) jak i na LFW (~0.98) uzyskują architektury Fudan. Następnie architektury SphereFace, FaceResNet, Light-CNN oraz CASIA uzyskują podobną jakość na zbiorze LFW (~0.97), jednak ich jakość na zbiorze walidacyjnym jest różna (od ~1.3 do ~1.0).  Można to zinterpretować, że cechy produkowane przez każdą z architektur mają podobną jakość, jedynie ostatnia warstwa kwalifikacyjna jest różnej jakości.  Jednakże tak nie jest, protokół BLUFR uwidacznia większe różnice w wynikach. Na prowadzenie wysuwa się SphereFace-64, a dopiero po nim są Fudan i FaceResNet. Warto zauważyć, że różnica 0.7% w LFW przeskalował się na różnicę 16% w protokole BLUFR-FAR 1% (porównując CASIA i SphereFace-64).&lt;br&gt;
Gdy weźmiemy pod uwagę podstawowe architektury, wówczas Fudan-Full wygrywa w każdym z analizowanych benchmarków. Jednakże widać, że DropOut znacząco poprawia jakość architektur, przede wszystkim poprzez zmniejszenia przeuczenia sieci. Do dokładniejszej analizy dodatków do archtektór przejdziemy w dalszy postach.&lt;/p&gt;
&lt;p&gt;Jeżeli porównamy czas potrzebny do uzyskania dobrego rezultatu, tutaj zdecydowanie najlepiej wypada FaceResNet-dropout, który jest tylko nieznacznie słabszy niż najlepszy model, ale uczył się prawie 3x krócej. Dlatego właśnie &lt;strong&gt;FaceResNet&lt;/strong&gt; (bez dropout) zostanie wybrany jako bazowa architektura do dalszych eksperymentów, wraz z podstawowym wynikiem (&lt;code&gt;baseline&lt;/code&gt;), który będziemy chcieli poprawić.&lt;/p&gt;
&lt;h1 id="codalej"&gt;Co dalej?&lt;/h1&gt;
&lt;p&gt;Patrząć ogólnie na wyniki, widać, że można osiągnąć wynik ~98% na LFW wykorzystując jedynie podstawowe techniki nauki sieci. Jeszcze 3 lata temu byłby to bardzo dobry wynik, obecnie wielu naukowców zaproponowało dodatkowe techniki zwiększające możliwości architektur. Czy na pewno one działają oraz czy jakiego zysku możemy się spodziewać? Tego dowiemy się w kolejnych postach, a w najbliżysz przyjrzymy się przygotowaniu danych uczących.&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Demystifying Face Recognition: Introduction</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="wstp"&gt;Wstęp&lt;/h1&gt;
&lt;p&gt;W internecie można znaleźć dużą liczbę artykułów oraz materiałów  przedstawiającą technikę rozpoznawania twarzy. Większość z nich opiera się na wytłumaczeniu pipeline rozpoznawania wraz z wykorzystam przygotowanych, open-source bibliotek. Dobrymi przykładami materiałów jest OpenFace, Face-Net czy Dlib. Każdy z nich dostarcza algorytmy wysokiej jakość. Jednak co jeśli okazuje się, że&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/demystifain-face-recognition-part-1/</link><guid isPermaLink="false">59c3a0a57820e9423c4a9fad</guid><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Thu, 21 Sep 2017 11:30:07 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/10/Face-Recognition-Pipeline---Page-1-.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h1 id="wstp"&gt;Wstęp&lt;/h1&gt;
&lt;img src="http://localhost:2368/content/images/2017/10/Face-Recognition-Pipeline---Page-1-.png" alt="Demystifying Face Recognition: Introduction"&gt;&lt;p&gt;W internecie można znaleźć dużą liczbę artykułów oraz materiałów  przedstawiającą technikę rozpoznawania twarzy. Większość z nich opiera się na wytłumaczeniu pipeline rozpoznawania wraz z wykorzystam przygotowanych, open-source bibliotek. Dobrymi przykładami materiałów jest OpenFace, Face-Net czy Dlib. Każdy z nich dostarcza algorytmy wysokiej jakość. Jednak co jeśli okazuje się, że algorytm nie spełnia naszych oczekiwań, jak go ulepszyć, jakie są możliwe metody? Oraz jak poprawnie przebadać algorytm, aby uzyskać odpowiedź, czy algorytm jest na pewno lepszy od poprzednio używanego? W serii blogów postaram się przybliżyć temat Rozpoznawania Twarzy, pokazać możliwe metody do zwiększenia jakości algorytmu oraz zweryfikować propozycje naukowców w kontrolowanych warunkach testowych. Najpierw przybliżmy temat testowania algorytmów rozpoznawania twarzy.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id="wiodcetechnikbadaniajakocialgorytmwrozpoznawaniatwarzy"&gt;Wiodące technik badania jakości algorytmów rozpoznawania twarzy&lt;/h1&gt;
&lt;p&gt;Temat rozpoznawania twarzy jest obecny w uczeniu maszynowym od dawna, jednak dopiero od 2008 widoczny jest postęp w osiągnięciu systemów dobrej jakości w warunkach niekontrolowanych. Przyczyną rozwoju technologii było przede wszystkim opublikowanie benchmarku o nazwie &lt;a href="http://vis-www.cs.umass.edu/lfw/"&gt;LFW&lt;/a&gt; (Labeled Faces in the Wild), który wyróżniał się przede wszystkim poprzez udostępnienie zdjęć zrobionych w niekontrolowanych warunkach. Główny test polegał na &lt;code&gt;Pair-Matching&lt;/code&gt;, czyli na porównaniu zdjęć dwóch osób oraz wyrokowanie czy to jest ta sama czy inna osoba. Obecnie wiele metod na LFW uzyskuje wyniki bliskie doskonałości, ~99.5%. Ale jednocześnie taki rezultat nie gwarantuje bardzo dobrej jakości systemu w innych warunkach. Dlatego w 2014 zaproponowano rozszerzenie LFW (&lt;a href="http://www.cbsr.ia.ac.cn/users/scliao/projects/blufr/"&gt;BLUFR&lt;/a&gt;) o protokół weryfiakcji, ale na odpowiednim poziomie FAR oraz o wiele większą liczbę par (~500 mln). Wprowadzono także protokół Identyfikacji, który odzwierciedla rozpoznanawania twarz w warunkach testowych.&lt;/p&gt;
&lt;p&gt;W kolejnym roku zaproponowano kolejny benchmark rozpoznawania twarzy o nazwie '&lt;a href="https://pdfs.semanticscholar.org/140c/95e53c619eac594d70f6369f518adfea12ef.pdf"&gt;IARPA Janus Benchmark A&lt;/a&gt;'. W kwestii protokołów, są one podobne do BLUFR. Główną różnicą jest testowanie na trudniejszych, specjalnie wybrancych zdjęciach i klatkach z video. Wprowadzono także testowanie za pomocą &lt;code&gt;template&lt;/code&gt;, zamiast pojedyńczych zdjęć. Czyli bazujemy na testowaniu w stylu &lt;code&gt;osoba v osoba&lt;/code&gt; zamiast &lt;code&gt;zdjęcie vs zdjęcie&lt;/code&gt;.&lt;br&gt;
 &lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition: Introduction" src="http://localhost:2368/content/images/2017/10/janus_template.png"&gt;
&lt;/p&gt;
&lt;p&gt;W 2017 roku wprowadzono rozszerzenie o nazwie &lt;a href="http://biometrics.cse.msu.edu/Publications/Face/Whitelametal_IARPAJanusBenchmark-BFaceDataset_CVPRW17.pdf"&gt;Janus Benchmark-B Face Dataset&lt;/a&gt; benchmarku, która poza zwiększeniem rozmiaru bazy danych, wprowadziło rozróżnienie pomiedzy testowaniem algorytmu na zdjęciach, na video lub na obu naraz. Dodatkowo wprowadza test klasteryzacji twarzy.&lt;/p&gt;
&lt;p&gt;Ostatnim popularnym benchmarkiem jest &lt;a href="http://megaface.cs.washington.edu/"&gt;MegaFace&lt;/a&gt;. Jak nazwa wskazuje, jest on benchmarkiem o dużo większej skali niż wszystkie inne, łącznie zawierając ponad 1 mln zdjęć. Istnieje dwa zbiory testowe, FaceScrub testujący zwykłą jakość algorytmu oraz FGNet, testujący age-invariant. Obie bazy są poddane testom na &lt;code&gt;distractors&lt;/code&gt;. Tak jak inne benchmarki (oprócz najstarszego LFW), posiada dwa protokoły: weryfikacji(~ 4 bilion par) oraz identyfikacji. W przypadku &lt;code&gt;Challange 1&lt;/code&gt;, naukowcy mają do wyboru dwa rodzaje testu: Small (dane uczące  &amp;lt; 0.5M) oraz Large. W przypadku &lt;code&gt;Challange 2&lt;/code&gt; do uczenia modelu mamy dostępną bazę danych udostępniona przez MegaFace o rozmiarze ~5M zdjęć. Umożliwia to testowanie przedewszystkim algorytmu uczącego, a nie bazy danych (jak to jest w przypadku  &lt;code&gt;Challange 1&lt;/code&gt;, gdzie każdy może stosować własną bazę danych).&lt;/p&gt;
&lt;p&gt;Bardzo dobre zestawienie wyników benchmarków zostało przedstawione w pracy &lt;a href="https://arxiv.org/abs/1511.02683"&gt;A Light CNN for Deep Face Representation with Noisy Labels&lt;/a&gt;, w której autor zaprezentował bardzo dokładne porównanie jakości metod na wielu benchmarki dedykowane rozpoznawania twarzy (oprócz wyżej wymienionych: &lt;a href="https://www.cs.tau.ac.il/~wolf/ytfaces/"&gt;YTF&lt;/a&gt;, &lt;a href="http://ieeexplore.ieee.org/document/4587572/"&gt;YTC&lt;/a&gt;, &lt;a href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html"&gt;Multi-PIE&lt;/a&gt;, &lt;a href="http://bcsiriuschen.github.io/CARC/"&gt;CACD-VS&lt;/a&gt;, &lt;a href="http://www.cbsr.ia.ac.cn/english/NIR-VIS-2.0-Database.html"&gt;CASIA 2.0 NIR-VIS&lt;/a&gt;,  &lt;a href="http://ieeexplore.ieee.org/document/6805594/"&gt;Celebrity-1000 &lt;/a&gt;). Analizując wyniki można zauważyć widać wyraźne różnice pomiędzy wynikami pomiędzy tymi samymi modeli w różnych testach (np. w jednym różnica pomiędzy modelami jest 0.5% a w inny 20%). W celu lepszego analizowania modelu, należałoby skupić się na benchmarkach wyraźnie pokazujących skoki jakości modeli, czyli należy unikać wyrokowanie co do jakości na podstawie tylko wyniku LFW, który jest najmniej wiarygodnym obecnie wynikiem.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id="wspczesnetechnikirozpoznawaniatwarzy"&gt;Współczesne techniki rozpoznawania twarzy&lt;/h1&gt;
&lt;p&gt;Główne metody rozpoznawania twarz opierają się na Deep Learningu. Naukowcy prześcigają się w metodach polepszania jakości za pomocą powiększana zbioru uczącego, zmiany architektur sieci czy zmiany funkcji celu. Za obecnie najlepszą technikę rozpoznawania twarzy uważam algorytm  &lt;strong&gt;Vocord&lt;/strong&gt;, zwycięzcę benchmarku MegaFace oraz obecnie drugi najlepszy algorytm wg raportu &lt;a href="https://www.nist.gov/sites/default/files/documents/2017/08/25/frvt_report_2017_08_25.pdf"&gt;NIST&lt;/a&gt;. Niestety nie znamy żadnych szczegółów na temat wykorzystanych technik do uzyskania tak dobrego wyniku.&lt;br&gt;
Jeżeli spojrzymy na inny benchmark, LFW, tutaj występuje wiele wyników osiągających wynik &amp;gt; 99.5%, których na których znamy więcej szczegółów implementacyjnych. Większość z nich operuje na bazach danych około 2M zdjęć oraz kilku sieciach neuronowych. Prace proponują także inne podejścia do uczenia poprzez zmianę ‘pipeline’ lub dodanie nowej funkcji celu. Jednakże, wg. mnie taki opis eksperymentów nie zbliża nas do osiągnięcia jeszcze lepszych wyników w przyszłości, ponieważ nie da się z nich wyciągnąć konkretnych wniosków na temat procesu uczenia jak np.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Czy twarz powinna być wyprofiliowana czy nie?&lt;/li&gt;
&lt;li&gt;Jakie techniki Data Augumentation pomagają?&lt;/li&gt;
&lt;li&gt;Jakie dodatki do architektur sieci pomagają?&lt;/li&gt;
&lt;li&gt;Jakie funkcje celu mają najkorzystniejszy wpływ na uczenie?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dzieje się tak, ponieważ każdy naukowiec używa swojej bazy danych, swojej koncepcji uczenia, nie zawsze dążą do osiągnięcia jak najlepszego wyniku, a do pokazania słuszności postawionej tezy.  Jest to oczywiście zrozumiałe podejście, ponieważ na tym polega nauka. Jednak w celach praktycznych, warto także zbadać obecne limity rozpoznawania twarzy, poprzez kombinację różnych technik.  W celu zaznajomienia się z  obecnymi wynikami na benchmarki LFW oraz YTF, prezentuję tabelę z pracy &lt;a href="https://ydwen.github.io/papers/WenCVPR17.pdf"&gt;SphereFace&lt;/a&gt;. Jest ona o tyle ciekawa, że ma podane także rozmiar bazy danych wykorzystanej do uczenia oraz liczbę wykorzystanych sieci neuronowych.&lt;br&gt;
 &lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition: Introduction" src="http://localhost:2368/content/images/2017/10/sphereface.png"&gt;
&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Nie są to wszystkie dostępne, wyniki. Jednak pozwalają one nam na ogólny pogląd na dokładność algorytmów. Obecnie najwyższy wynik na LFW to &lt;strong&gt;99.83%&lt;/strong&gt;, zarejestrowany przez firmę Glasix z następującym opisem metody:&lt;br&gt;
Brief author's description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We followed the Unrestricted, Labeled Outside Data protocol. The ratio of White to Black to Asian is 1:1:1 in the train set, which contains 100,000 individuals and 2 million face images. We spent a week training the networks which contain a improved resnet34 layer and a improved triplet loss layer on a Dual Maxwell-Gtx titan x machine with a four-stage training method.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition: Introduction" src="http://vis-www.cs.umass.edu/lfw/lfw_unrestricted_labeled_zm.png"&gt;
&lt;/p&gt;
&lt;p&gt;Jeżeli chcecie zobaczyć więcej wyników, zapraszam na strony:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://vis-www.cs.umass.edu/lfw/results.html"&gt;LFW&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://megaface.cs.washington.edu/results/facescrub.html"&gt;MegaFace&lt;/a&gt;&lt;br&gt;
 &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="celseriipostw"&gt;Cel serii postów&lt;/h2&gt;
&lt;p&gt;Głównym celem serii postów, będzie opracowanie pełnego algorytmu, który będzie jak najlepiej działał na ogólno dostępnych benchmarkach, przy czym docelowym testem będzie &lt;a href="http://megaface.cs.washington.edu/participate/challenge.html"&gt;MegaFace Challange 1 - Small&lt;/a&gt; oraz &lt;a href="http://megaface.cs.washington.edu/participate/challenge2.html"&gt;MegaFace Challange 2&lt;/a&gt;. W tym celu będą testowanie każdy element z pipelinu, min. następujące idee:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dobór architektury sieci&lt;/li&gt;
&lt;li&gt;Techniki Data Augumentation&lt;/li&gt;
&lt;li&gt;Wybór algorytmu optymalizacji sieci&lt;/li&gt;
&lt;li&gt;Funkcje celu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Aby mieć możliwość wyciągnięcia wniosków z eksperymentów zostaną założone ograniczenia oraz założenia początkowe, ułatwiające analizę wyników.&lt;br&gt;
Ograniczenia:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wszystkie algorytmy będą operować na bazie danych &lt;a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html"&gt;CASIA-WebFace&lt;/a&gt; (0.5M zdjęć, 10k ludzi)&lt;/li&gt;
&lt;li&gt;90% zbioru danych będzie stanowić zbiór treningowy, 10% walidacyjny&lt;/li&gt;
&lt;li&gt;W czasie testowania modelu, dla każdego zdjecia będą ekstrakowane tylko jeden zbiór cech (czyli nie będziemy wykorzystywać &lt;a href="https://github.com/happynear/NormFace/blob/master/MirrorFace.md"&gt;‘mirror-trick’&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zawsze będzie wykorzystywany tylko jeden egzemplarz architektury sieci (czyli brak łączenia cech z kilku sieci)&lt;/li&gt;
&lt;li&gt;Początkowy LR został wybrany z setu: &lt;em&gt;0.1, 0.04, 0.01, 0.001.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Do redukcji LR został wykorzystana algorytm detekcji &lt;code&gt;Plateu&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Założenia początkowe:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Architektury sieci będą używały obrazów wykrytych oraz wyprofiliwanych za pomocą algorytmu MTCNN. Ich rozmiar wynosi 112x96.&lt;/li&gt;
&lt;li&gt;Podstawową techniką Data Augumentation podczas uczenia będzie ‘mirror’&lt;br&gt;
 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/10/sprite_image_69_w_111_h_130.png" alt="Demystifying Face Recognition: Introduction"&gt;&lt;br&gt;
 &lt;/p&gt;
&lt;p&gt;Rozmiar bazy danych jak i obrazów wejściowych został tak dobrany, aby umożliwić uzyskanie wysokiej jakości metody, a jednocześnie skrócić czas jej działania. Liczba eksperymentów potrzebna do uzyskania końcowego wyniku jest ogromna, a moc obliczeniowa ograniczona.&lt;/p&gt;
&lt;p&gt;Jako podstawowy wyznacznik jakości metody będą brane pod uwagę dwa wyniki: LFW, LFW-BLUFR. Dodatkowo dla najlepszego modelu z danego postu przeprowadzę testy na IARPA Janus Benchmark-A oraz MegaFace.&lt;/p&gt;
&lt;p&gt;Każdy z eksperymentów będzie porównywany do &lt;code&gt;baseline&lt;/code&gt;, czyli wybranego sposobu (dane-&amp;gt;architektura-&amp;gt;funka celu), która uzyskała swój wynik dość prostymi metodami. Pozwoli to nam na ocenienie, czy nowa, zaproponowana technika wpływa na jakość algorytmu pozytywnie.&lt;/p&gt;
&lt;p&gt;To na tyle w wprowadzeniu, w następnym poście zajmiemy się stworzeniem &lt;code&gt;baseline&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;</content:encoded></item></channel></rss>