<!DOCTYPE html>
<html âš¡>
<head>
    <meta charset="utf-8">

    <title>Demystifying Face Recognition II: Baseline</title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../../../../favicon.ico">

    <link rel="shortcut icon" href="../../../../../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="BLCV- Bartosz Ludwiczuk Computer Vision" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Demystifying Face Recognition II: Baseline" />
    <meta property="og:description" content="Getting baseline for Face-Recognition" />
    <meta property="og:url" content="http://localhost:2368/2017/10/27/demystifying-face-recognition-i-baseline/" />
    <meta property="og:image" content="http://localhost:2368/content/images/2017/10/faceresnet-1.png" />
    <meta property="article:published_time" content="2017-10-27T11:19:00.000Z" />
    <meta property="article:modified_time" content="2017-10-27T11:44:31.000Z" />
    <meta property="article:tag" content="face-recogition" />
    
    <meta property="article:publisher" content="https://www.facebook.com/bartosz.ludwiczuk" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Demystifying Face Recognition II: Baseline" />
    <meta name="twitter:description" content="Getting baseline for Face-Recognition" />
    <meta name="twitter:url" content="http://localhost:2368/2017/10/27/demystifying-face-recognition-i-baseline/" />
    <meta name="twitter:image" content="http://localhost:2368/content/images/2017/10/faceresnet-1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Bartosz Ludwiczuk" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="face-recogition" />
    <meta property="og:image:width" content="689" />
    <meta property="og:image:height" content="93" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "BLCV- Bartosz Ludwiczuk Computer Vision",
        "logo": "http://localhost:2368/content/images/2017/10/logo.png"
    },
    "author": {
        "@type": "Person",
        "name": "Bartosz Ludwiczuk",
        "url": "http://localhost:2368/author/bartosz/",
        "sameAs": []
    },
    "headline": "Demystifying Face Recognition II: Baseline",
    "url": "http://localhost:2368/2017/10/27/demystifying-face-recognition-i-baseline/",
    "datePublished": "2017-10-27T11:19:00.000Z",
    "dateModified": "2017-10-27T11:44:31.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:2368/content/images/2017/10/faceresnet-1.png",
        "width": 689,
        "height": 93
    },
    "keywords": "face-recogition",
    "description": "Getting baseline for Face-Recognition",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 1.8" />
    <link rel="alternate" type="application/rss+xml" title="BLCV- Bartosz Ludwiczuk Computer Vision" href="../../../../../rss/index.html" />

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,600,400" />
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-iframe" src="https://cdn.ampproject.org/v0/amp-iframe-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../../../../index.html">BLCV- Bartosz Ludwiczuk Computer Vision</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Demystifying Face Recognition II: Baseline</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../../../../author/bartosz/index.html">Bartosz Ludwiczuk</a></p>
                    <time class="post-date" datetime="2017-10-27">2017-10-27</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="http://localhost:2368/content/images/2017/10/faceresnet-1.png" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <div class="kg-card-markdown"><h2 id="testofdifferentnetworkarchitectures">Test of different network architectures</h2>
<p>According to assumptions, the database is chosen (CASIA-WebFace), input image is preprocessed (112x96, MTCNN), only mirror used as Data Augmentation and for learning the CrossEntropy loss will be used. The last lacking element in the pipeline is network architecture. Last years was really abounded in many diverse ideas about creating the architectures like <code>ResNet</code>, <code>Inception</code> or <code>DenseNet</code>. Additional, the community of Face Recognition was also introducing their own architectures like <code>FaceResNet</code>, <code>SphereNet</code>,<code>LightCNN</code> or <code>FudanNet</code>. Currently we will look closer to the later one as we know their performance and low computation requirements.<br></br>
We will also include some older architectures to see if it is really true then the new ones works much better than architectures from 2014 or earlier, we choose <code>LeNet</code>, <code>DeepID</code>, <code>DeepID2+</code> and <code>CASIA</code>.<br></br>
This is not the final choice of the architecture, we just want to get a reasonable baseline, which will accompany us with all others test. There are so many test, because we want to make sure, that my current pipeline works well and if my implementation match results from papers.</p>
<h2 id="descriptionofarchitectures">Description of Architectures</h2>
<p><strong><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">LeNet</a></strong> - the most popular convolutional architecture. Input image: 28x24.<br></br>
â€‹<br></br>
<strong><a href="http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf">DeepID</a></strong> - One of the first specialized networks used for <code>Face Recognition</code>. Comparing to <code>LeNet</code>, it have more filters and final feature comes from merging data from two layers. Input image: 42x36.<br></br>
â€‹<br></br>
<strong><a href="https://arxiv.org/abs/1412.1265">DeepID2+</a></strong> - Extension of <code>DeepID</code>, have much more number of filters and features size is now 512. Input image:56x48.<br></br>
â€‹<br></br>
<strong><a href="https://arxiv.org/abs/1411.7923">Casia-Net</a></strong> - Architecture proposed after success of VGG and GoogLeNet. It use concept of kernel 3x3 and <code>Average Pooling</code>.<br></br>
â€‹<br></br>
<strong><a href="https://arxiv.org/abs/1511.02683">Light-CNN</a></strong>- The author propose to use <code>MFM</code> as a activation function, which is extension of <code>MaxOut</code>. In his experiments it is better than <code>ReLU</code>, <code>ELU</code> or even  <code>PReLU</code>.<br></br>
â€‹<br></br>
<strong><a href="https://arxiv.org/abs/1611.08976">FaceResNet</a></strong> - Architecture proposed by author of CenterLoss and RangeLoss, which use <code>residual connection</code>, like in <code>ResNet</code>. But it does not use BatchNorm  and replaces the <code>Relu</code> activation functions with the <code>PRELu</code> functions.<br></br>
â€‹<br></br>
<strong><a href="https://arxiv.org/abs/1704.08063">SphereFace</a></strong> - New version of <code>FaceResNet</code> which mainly replace each MaxPool by Convolution with stride equal to 2.<br></br>
â€‹<br></br>
<strong><a href="http://www.yugangjiang.info/publication/icmr17-face.pdf">Fudan-Arch</a></strong> -Idea of <code>FaceResNet</code> but with Batchnorm.</p>
<p>Most of the above architecture have also <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">DropOut</a> inside, other have own regularization method. If we just want to replicate the results as stated at paper, we would still not be able to compare such results to each other because of different settings. This is why we will completely ignore any special regularization method (like CenterLoss) and here will be two experiments for each architecture: with and without DropOut. This would also help to validate the current implementation with the results from papers.<br></br>
To evaluate each network we will use its qualitative results (accuracy and loss value) and time-to-score. The model of each architect was chosen based on the model with the lowest validation loss, ie the result of the LFW did not affect the choice of the model, even though the models achieved better results in another epoch.</p>
<h2 id="results">Results</h2>
<p align="center"><b>CASIA Training and LFW</b></p>
<p align="center">
<amp-iframe width="840" height="1000" frameborder="0" src="https://plot.ly/~melgor89/92.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
</p>
<amp-iframe width="800" height="400" frameborder="0" src="https://plot.ly/~melgor89/98.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<amp-iframe width="800" height="400" frameborder="0" src="https://plot.ly/~melgor89/97.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<p>First of all, let's look closer to architectures with DropOut. There is not clean winner here, <strong>Fudan-Full</strong>, <strong>SphereFace64</strong> and <strong>Light-CNN29</strong> are overall comparable, but each of them dominate at one of the given benchmark (validation loss, LFW, BLUFR). Very close to them in <strong>FaceResNet</strong>, which was training much faster. It is very interesting that many network achieve &gt; 97% at LFW, however BLUFR protocols show us the real difference in quality. For example, difference in 0.7% at LFW between <strong>CASIA</strong> and <strong>SphereFace64</strong> translate to 16% in BLUFR-FAR 1%.<br></br>
What about architectures without any regularization? Here the clear winner is <strong>Fudan-Full</strong>, followed by <strong>SphereFace64</strong>  and <strong>FaceResNet</strong>. From the intuition, it look like that BatchNorm at <strong>Fudan-Full</strong> helped at lot as it behaves like a regularizator. From such comparison we can also deduce which architecture is good for testing any new  Data Augmentation technique or new loss, because it would show us even small gain. In our case it is <strong>Light-CNN29</strong> which overfit a lot.</p>
<p>For detailed analyse we choose best models: <strong>FaceResNet</strong>, <strong>Light-CNN29</strong>, <strong>SphereFace64</strong> and <strong>FudanFull</strong>.</p>
<p align="center"><b>IJB-A</b></p>
<p align="center">
<amp-iframe width="800" height="260" frameborder="0" src="https://plot.ly/~melgor89/106.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<amp-iframe width="800" height="400" frameborder="0" src="https://plot.ly/~melgor89/103.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
</p>
<p>Overall here <strong>Light-CNN29</strong> is the winner, which lose at only Rank-1 benchmark. But <strong>SphereFace64</strong>  is breathing down its neck by being just slightly worse. The results from <strong>FundanFull</strong> are really bad,  not sure what is the reason for that.</p>
<p align="center"><b>Mega Face</b></p>
<p align="center">
<amp-iframe width="500" height="200" frameborder="0" src="https://plot.ly/~melgor89/104.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
</p>
<amp-iframe width="800" height="400" frameborder="0" src="https://plot.ly/~melgor89/105.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<p></p>
<p>In MageFace, identification protocol is winner by <strong>FudanFull</strong> while verification protocol is taken by <strong>Light-CNN29</strong> (where <strong>FudanFull</strong> is again the weakest)</p>
<h2 id="baselinemodel">Baseline Model</h2>
<p>Summarizing, if we want to choose best architecture among tested, the <strong>Light-CNN29</strong> would be the best with  <strong>Sphere64</strong> just right behind. <strong>FudanFull</strong> works nice, but in some scenario its accuracy is too low. This is our podium. Looking closer to the this architectures, the common thing is using residual connection. But they vary at activation function, using Pool vs Convolution with stride equal 2 and using BatchNorm. So maybe they are not best possible architectures? We will leave this question for future tests.</p>
<p>When we compare the results from current implementation with the results from paper, most of them matched target accuracy. The only exception is SphereFace, which without DropOut overfit, although the original version does not have it.</p>
<p>When we compare the time needed for getting best results, this is definitely the best place for <strong>FaceResNet</strong>, which is only slightly weaker than the best model, but it learned almost 3x shorter. This is why <strong>FaceResNet</strong> is chosen as baseline architecture. He will accompany us throughout the series named <code>Face Recognition</code>. Specifically, both <strong>FaceResNet</strong> will be used, depending of scenario: when we will be reducing overfiting by new technique, we will use raw architecture, in other case DropOut will be used.</p>
<h2 id="whatnext">What next?</h2>
<p>Looking into the results it look like that getting ~98% on LFW using only basic technique for learning is easy. This results would be among the best 3 years ago, but currently it is ~1.5% behind state-of-the-art. In MegaFace in even worse, because our results is 20% lower using same dataset.<br></br>
How we can boost accuracy of our model? A lot of researcher propose their own technique, but will they work in our case? What boost can we gain? We will learn this in the next post, and in the near future we will look at the aspect of noise in the learning data.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient-Based Learning Applied to Document Recognition</a></li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf">Deep Learning Face Representation from Predicting 10,000 Classes</a></li>
<li><a href="https://arxiv.org/abs/1412.1265">Deeply learned face representations are sparse, selective, and robust</a></li>
<li><a href="https://arxiv.org/abs/1411.7923">Learning Face Representation from Scratch</a></li>
<li><a href="https://arxiv.org/abs/1511.02683">A Light CNN for Deep Face Representation with Noisy Labels</a></li>
<li><a href="https://arxiv.org/abs/1611.08976">Range Loss for Deep Face Recognition with Long-tail</a></li>
<li><a href="https://arxiv.org/abs/1704.08063">SphereFace: Deep Hypersphere Embedding for Face Recognition</a></li>
<li><a href="http://www.yugangjiang.info/publication/icmr17-face.pdf">Multi-task Deep Neural Network for Joint Face Recognition and Facial Aribute Prediction</a></li>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
</ul>
</div>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../../../../index.html">BLCV- Bartosz Ludwiczuk Computer Vision</a> &copy; 2017</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
</html>
